# Speech-to-Emotion-Recognition

This project focuses on detecting various emotions based on our speech. This project integrates Convolution Neural Networks (CNNs) and Long-Short Term Memory (LSTM) networks. The primary objective is to develop a precise and efficient system for automatically identifying and categorizing emotions conveyed through speech signals.

The datasets used in this project are:
- Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D)
- Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)
- Surrey Audio-Visual Expressed Emotion (SAVEE)
- Toronto Emotional Speech Set (TESS)

CREMA-D : https://www.kaggle.com/datasets/ejlok1/cremad<br/>
RAVDESS : https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio<br/>
SAVEE : https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess<br/>
TESS : https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee<br/>



